{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install -q kaggle\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder\\\n",
    "          .master(\"local[10]\")\\\n",
    "          .appName(\"app-apache-spark\")\\\n",
    "          .config('spark.ui.port','4050')\\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargar dataset\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download mkechinov/ecommerce-events-history-in-cosmetics-shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descomprimir el.zip del dataset\n",
    "for file in os.listdir():\n",
    "  print(file)\n",
    "  if file.endswith('.zip'):\n",
    "    zip_ref = zipfile.ZipFile(file,'r')\n",
    "    zip_ref.extractall()\n",
    "    zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar el dataset y crear dataframe\n",
    "df = spark.read.options(header='True',inferScheme='True').csv('*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:=============================>                          (10 + 8) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20692840\n",
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"event_type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = df.select(['user_session']).filter(\"event_type='cart' AND product_id=4958\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "Obtener que otros productos tambien fueron agregados al carrito junto al producto con id 4958 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = df.select(['user_session']).filter(\"event_type='cart' AND product_id=4958\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5759760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products = df.select(['product_id']).filter(\"event_type='cart' AND product_id<>4958\").filter( df[\"user_session\"].isin(sessions[\"user_session\"]) )\n",
    "print(products.count())\n",
    "products = products.distinct()\n",
    "print(products.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products.write.mode(\"overwrite\").csv('resultado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(s):\n",
    "  if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "    return [ ( s[\"product_id\"],1 ) ]\n",
    "  return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 276\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:571)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:534)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "[Stage 96:==================================================>     (17 + 2) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('5842214', 13)\n",
      "('5842215', 10)\n",
      "('5842242', 7)\n",
      "('5842212', 3)\n",
      "('5842205', 12)\n",
      "('5842220', 6)\n",
      "('5922103', 1)\n",
      "('5842213', 12)\n",
      "('5842241', 6)\n",
      "('5842219', 9)\n",
      "('5842231', 16)\n",
      "('5922106', 3)\n",
      "('5842202', 17)\n",
      "('5842203', 13)\n",
      "('5922109', 1)\n",
      "('5842233', 4)\n",
      "('5922115', 1)\n",
      "('5922123', 1)\n",
      "('5922121', 1)\n",
      "('5842232', 20)\n",
      "('5842268', 131)\n",
      "('5922107', 5)\n",
      "('5922119', 2)\n",
      "('5922110', 3)\n",
      "('5922122', 1)\n",
      "('5842235', 12)\n",
      "('5842258', 20)\n",
      "('5844573', 3)\n",
      "('5922097', 1)\n",
      "('5842223', 12)\n",
      "('5846097', 5)\n",
      "('5846098', 21)\n",
      "('5922118', 9)\n",
      "('5842224', 9)\n",
      "('5922124', 1)\n",
      "('5922112', 2)\n",
      "('5922104', 2)\n",
      "('5842221', 9)\n",
      "('5842204', 17)\n",
      "('5842217', 11)\n",
      "('5842234', 5)\n",
      "('5842206', 2)\n",
      "('5844571', 20)\n",
      "('5842240', 12)\n",
      "('5922117', 6)\n",
      "('5842222', 25)\n",
      "('5842225', 6)\n",
      "('5842226', 6)\n",
      "('5842227', 11)\n",
      "('5922114', 1)\n",
      "('5922108', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines = df.rdd.flatMap(myFunc).reduceByKey(lambda a, b: a + b)\n",
    "for element in lines.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.saveAsTextFile(\"resultado2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:========================================>              (14 + 5) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|product_id|   brand|price|\n",
      "+----------+--------+-----+\n",
      "|   5767003|kosmekka|99.84|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"SELECT DISTINCT(product_id), brand, price FROM data WHERE price=(SELECT MAX(price) FROM data)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "sbin ./start-master.sh\n",
    "sbin ./start-worker.sh spark://127.0.0.1:7077\n",
    "bin ./spark-submit --master spark://MacBook-Pro-de-Jhon.local:7077 /Users/sebastian/ApacheSpark/ipynb/streaming.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# la regresión logística es un tipo de análisis de regresión utilizado \n",
    "# para predecir el resultado de una variable categórica \n",
    "# (una variable que puede adoptar un número limitado de categorías) \n",
    "# en función de las variables independientes o predictoras.\n",
    "\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model1.extractParamMap())\n",
    "\n",
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "# Specify multiple Params.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "# Change output column name\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)  # type: ignore\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "print(model2.extractParamMap())\n",
    "\n",
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "prediction = model2.transform(test)\n",
    "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.myProbability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHX - GRAPHFRAMES\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
